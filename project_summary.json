{
  "project_name": "OLM Pipeline - Frozen VAE Latent Dynamics",
  "version": "1.0.0",
  "status": "Major Milestone - First Functioning OLM Model",
  "description": "First functioning model on the OLM (Object-Level Manipulation) pipeline that turns raw video into predictive latent state using a frozen VAE and three small LSTMs. This represents a major stepping stone towards completing the OLM project.",

  "core_achievement": {
    "milestone": "First functioning model with the OLM pipeline",
    "significance": "Major stepping stone towards completing OLM project",
    "approach": "Frozen VAE + three-stage LSTM architecture for video state extraction and prediction"
  },

  "architecture": {
    "type": "Video State Extraction Pipeline",
    "components": [
      {
        "name": "Frozen VAE",
        "function": "Stable frame-to-latent encoding/decoding using posterior mean",
        "source": "Stable Diffusion VAE",
        "status": "frozen",
        "scaling": "consistent scaling factor"
      },
      {
        "name": "PatternLSTM",
        "function": "Aggregates short window of latents for pattern detection",
        "parameters": "512 hidden dim, 2 layers, 16 buffer size",
        "status": "frozen",
        "purpose": "temporal aggregation"
      },
      {
        "name": "CompressionLSTM",
        "function": "Compresses PatternLSTM outputs while avoiding variance collapse",
        "parameters": "256 compressed dim, 2 layers",
        "status": "trainable",
        "purpose": "compression"
      },
      {
        "name": "CentralLSTM",
        "function": "Predicts Δz for next step with step controller scaling",
        "parameters": "512 hidden dim, 3 layers",
        "status": "trainable",
        "purpose": "prediction"
      }
    ],
    "design_philosophy": "Clean separation of sensing (frozen VAE), temporal aggregation (Pattern/Compression), and prediction (Central)"
  },

  "performance_metrics": {
    "one_step_prediction": {
      "psnr_db": "25-26",
      "ssim": "~0.99",
      "latent_mse": "2.6e-3 to 3.5e-3",
      "causality": "cos(ẑₜ₊₁, zₜ₊₁) > cos(zₜ, zₜ₊₁) consistently"
    },
    "open_loop_rollouts": {
      "5_step": {
        "psnr_db": "17.6-18.4",
        "ssim": "0.94-0.95"
      },
      "10_step": {
        "psnr_db": "16.1-17.0",
        "ssim": "0.93-0.94"
      },
      "characteristics": "monotonic decay, no blow-ups"
    },
    "stability": {
      "compressor_health": "per-dim std slowly increases, no collapse",
      "central_lstm": "norms bounded with gradient clipping enabled"
    }
  },

  "telemetry_system": {
    "frame_metrics": ["PSNR", "SSIM"],
    "latent_metrics": ["MSE", "cosine similarities", "delta norms"],
    "compression_metrics": ["compressor variance"],
    "rollout_analysis": ["rollout curves"],
    "state_monitoring": ["state norms", "gradient norms"],
    "temporal_analysis": ["scheduled sampling", "teacher forcing ratios"]
  },

  "advantages": {
    "debuggability": "Easier to debug than end-to-end approaches",
    "verification": "Easier to verify no time leakage",
    "iteration_speed": "Fast to iterate on components",
    "state_extraction": "Practical way to extract structured state from environment",
    "latent_properties": "Latent is compact, hashable, and predictable over short horizons"
  },

  "roadmap": {
    "immediate_next": {
      "background_plate_lora": {
        "type": "decoder-side LoRA",
        "rank": "4-8",
        "conditioning": "identity token",
        "training": "supervised by plate frames",
        "inference": "toggle to remove subject",
        "purpose": "world without me functionality"
      }
    },
    "future_enhancements": [
      {
        "name": "ICMI (Identity-Conditioned Matting + Inpainting)",
        "purpose": "Moving cameras and non-static backgrounds",
        "components": ["matting head for subject mask", "latent inpainting with temporal loss"]
      },
      {
        "name": "Subject Replacement",
        "requirements": ["stable removal", "subject-LoRA for target identity", "pose driving from keypoints", "lighting integration", "temporal consistency"]
      },
      {
        "name": "OLM Integration",
        "components": ["novelty driver", "live-VAE adaptation", "verified backbone integration"]
      }
    ]
  },

  "use_cases": [
    "Privacy-preserving streaming",
    "Data collection without operator presence",
    "AR compositing",
    "Controllable training data for downstream policies",
    "Environment state extraction for agents"
  ],

  "technical_details": {
    "processing_rate": "24 FPS",
    "hardware_requirements": ["CUDA-compatible GPU recommended", "webcam or video source"],
    "model_size": "Small, inspectable pipeline",
    "training": "Real-time learning capability",
    "scheduled_sampling": "Teacher forcing with decay",
    "step_controller": "Δz scaling to match observed motion"
  },

  "files_structure": {
    "core_files": [
      {
        "file": "AI.py",
        "purpose": "Main pipeline orchestration, online loop, step controller, metrics emission"
      },
      {
        "file": "vae_processor.py",
        "purpose": "Frozen VAE encode/decode with posterior mean and consistent scaling"
      },
      {
        "file": "lstm_models.py",
        "purpose": "PatternLSTM, CompressionLSTM, CentralLSTM, Δz head, scheduled sampling"
      },
      {
        "file": "camera_gui.py",
        "purpose": "Live view with status and metrics display"
      }
    ],
    "config_files": [
      {
        "file": "ai_config.json",
        "purpose": "LSTM model parameters, training configuration"
      },
      {
        "file": "camera_config.json",
        "purpose": "Camera settings and AI pipeline controls"
      }
    ],
    "output_directories": [
      {
        "directory": "ai_logs/",
        "contents": ["metrics.jsonl", "benchmarks.jsonl"]
      },
      {
        "directory": "vae/",
        "contents": ["VAE model files", "config.json"]
      }
    ]
  },

  "limitations": {
    "horizon": "Short-horizon predictor by design",
    "rollout_degradation": "Rollouts degrade without multi-step objectives",
    "motion_handling": "Δz can overshoot on high-motion frames (mitigated by controller)",
    "throughput": "Modest throughput, emphasis on correctness/telemetry"
  },

  "ethical_considerations": {
    "identity_editing": "Consent required for identity replacement",
    "output_labeling": "Clear labeling for edited outputs required",
    "technical_safeguards": "Small LoRA size to minimize background drift",
    "localization": "Decoder bottleneck + first upsample only"
  },

  "validation_approach": {
    "separation_testing": "Verified clean separation of sensing, compression, and prediction",
    "stability_analysis": "No divergence in open-loop rollouts",
    "causality_verification": "Temporal causality consistently maintained",
    "compression_health": "No variance collapse in compression stage"
  },

  "significance_for_olm": {
    "foundation": "Establishes that OLM's sensing→compression→prediction stack can extract environment state",
    "learning_validation": "Proves short-horizon dynamics learning without large end-to-end training",
    "plugin_architecture": "Stable, debuggable backbone for identity-aware editing",
    "exploration_ready": "Framework ready for novelty-driven exploration integration",
    "adaptation_capable": "Architecture supports live VAE adaptation"
  },

  "reproducibility": {
    "code_availability": "Complete code structure available",
    "training_recipe": "Minimal training recipe ready for LoRA implementation",
    "layer_specifications": "Detailed layer targets for conditioning available",
    "logging_system": "Comprehensive telemetry for verification"
  }
}